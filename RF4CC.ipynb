{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for An Alignment Cost-Based Classification of Log Traces\n",
    "\n",
    "> Paper : <u>An Alignment Cost-Based Classification of Log Traces Using Machine-Learning </u><br>\n",
    "> Date : June 2020 <br>\n",
    "> Authors : <i>Mathilde Boltenhagen, Benjamin Chetioui, and Laurine Huber  </i> <br>\n",
    "\n",
    "This notebook is organized as follow : <br> <br>\n",
    "<b>0. Fitness function </b> \n",
    "- The lower bound fitness is a good contribution of the paper, please see the paper for more details. <br>\n",
    "\n",
    "<b>1. Preprocessing the data:</b>\n",
    " - A function ```cleanDataForRF``` contains all the preprocessing steps. It reads the file, create the B.O.W. and the targets. \n",
    " \n",
    "<b>2. Model: </b> \n",
    "   - This is just the setting of the random forest learning model. \n",
    "   \n",
    "<b>3. Cross-Validation of the method : </b>\n",
    " - A function ```runKFoldForRF``` runs a Kfold method to fit and test the model on the B.O.W.\n",
    " \n",
    "<b> 4. Train and Test :</b>\n",
    "- Train and test for many  m_AC values\n",
    "\n",
    "## 0. Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import time\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow import enable_eager_execution\n",
    "enable_eager_execution() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commented lines were used when the frequency of the variants was incorporated in the computation of fitness.\n",
    "In fact, we decided to compute the fitness and lower bound for fitness on the variants only due to the understanding of the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(packageForFitness, minRunLength):\n",
    "    '''\n",
    "    This function computes the fitness for all the sequences. \n",
    "    @sequences: the sequences of words\n",
    "    @costs: the real alignment cost\n",
    "    @minRunLength: the minimal run in the alignment dataset\n",
    "    '''\n",
    "    sumTraceFitness = 0\n",
    "    totTraces = 0 \n",
    "    for i in packageForFitness.index:\n",
    "        sumTraceFitness += (1 - (packageForFitness.realCosts[i] / ( packageForFitness.lengths[i]  + minRunLength )))#*packageForFitness.freqs[i]\n",
    "        #totTraces += packageForFitness.freqs[i]\n",
    "        totTraces +=1\n",
    "    return sumTraceFitness / totTraces\n",
    "\n",
    "def LB_fitness(packageForFitness, minRunLength, m_AC, indices):\n",
    "    '''\n",
    "    This function computes lower bound of the fitness given in the paper. \n",
    "    @sequences: the sequences of words\n",
    "    @minRunLength: the minimal run in the alignment dataset\n",
    "    @m_AC: needed for the lowerbound formula\n",
    "    @indices: if we compute the lower bound, then we don't iterate on all the traces but only the positives. \n",
    "    '''\n",
    "    sumTraceFitness = 0\n",
    "    totTraces = 0 \n",
    "    for i in indices:\n",
    "        sumTraceFitness += (1 - ((m_AC) / ( packageForFitness.lengths[i] + minRunLength )))#*packageForFitness.freqs[i]\n",
    "    for i in packageForFitness.index:\n",
    "        #totTraces += packageForFitness.freqs[i]\n",
    "        totTraces +=1\n",
    "    return sumTraceFitness / totTraces\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the data\n",
    "\n",
    "This function takes as input an alignment dataset and its Maximal Alignment Cost and clean the data in order to get a B.O.W. and the target classes. Please, see the definition of Maximal Alignment Cost Classification for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object as a GLOBAL VARIABLE, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataForRF(dataFile,m_AC,vectorizer_already_trained=None): \n",
    "    '''\n",
    "    Reads the file (1), specifies the target classes (2) and prepare the Bag of Words(3).\n",
    "    @dataFile: (String) filename of the alignment dataset\n",
    "    @m_AC: (int) maximal alignment cost classifier\n",
    "    '''\n",
    "    # ---- (1) yes, a bit of copy/paste... Read the file \n",
    "    data = pd.read_csv(dataFile,sep = \";\", \n",
    "                   names = [\"traces\",\"tracesWithMoves\",\"runs\",\"runsWithMoves\",\"costs\",\"frequencies\"])\n",
    "    \n",
    "    # ---- (2) create the positive and negative target depending on the m_AC parameter\n",
    "    # alignment cost which interests us is greater than 10000 (other costs are just silent moves)\n",
    "    # set the fitting to tmp_pos to set them latter to 1\n",
    "    y = ((data[\"costs\"] / 10000) / (m_AC+1)).astype(int)\n",
    "    max_y = y.max()\n",
    "    y = y.replace(0,\"tmp_pos\")\n",
    "    y = y.replace(range(1,max_y + 1), 0)\n",
    "    y = y.replace(\"tmp_pos\",1)\n",
    "    y = np.eye(2)[y.to_numpy().reshape(-1)]\n",
    "    \n",
    "    # ---- (3) prepare the Bag of Words\n",
    "    traces_to_matrix = data.traces.str.split(\":::\",expand=True,)\n",
    "    # this line takes the matrix of words, and transformed it to a list of sentences\n",
    "    data_to_fit = [' '.join( [e.replace(\" \",\"\") for e in filter(None,a)]) for a in traces_to_matrix.values.tolist()]\n",
    "    \n",
    "    if vectorizer_already_trained :\n",
    "        x = vectorizer.transform(data_to_fit).toarray()\n",
    "    else :\n",
    "        # then we can transform our data with the counter (it's like a one-hot-encode, right?)\n",
    "        x = vectorizer.fit_transform(data_to_fit).toarray()\n",
    "    \n",
    "    # for fitness computation\n",
    "    minLengthRun= len(data.runs.str.split(\":::\").min())\n",
    "    lengths = data.traces.str.split(\":::\",expand=False,).str.len()-1\n",
    "    realCosts = (data[\"costs\"] / 10000).astype(int)\n",
    "    packageForFitness = pd.DataFrame({\"lengths\":lengths, \"realCosts\": realCosts, \"freqs\": data.frequencies})\n",
    "    \n",
    "    return x, y, packageForFitness, minLengthRun, m_AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 5, 5, 0],\n",
       "        [1, 0, 0, ..., 5, 3, 0],\n",
       "        [1, 0, 0, ..., 3, 0, 0]], dtype=int64), array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of use\n",
    "x, y, packageForFitness, minLengthRun, m_AC = cleanDataForRF(\"alignments/A_2012_im.csv\",4)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "Just a sklearn call. In fact, the call will be in the KFold so we can keep the best model of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Cross-Validation of the method \n",
    "\n",
    "From ```KFold``` of sklearn, we do a cross-validation on the training set. The outputs are average of the accuracy and the loss (binary-cross entropy). We use the ```BinaryCrossentropy``` function of <b> Tensorflow </b> to be consistent with the other experiment. \n",
    "\n",
    "- In order to see if positives or negatives items have better results, we give the results for `all` the test items, only the `positive` items and only the `negative` items. \n",
    "- `loss`, `acc` and `accLossPercentage` functions have been implemented in order to reduce number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(forest, x_test, y_test):\n",
    "    y_test_predict_with_proba = forest.predict_proba(x_test)[1]\n",
    "    return  BinaryCrossentropy()(y_test,y_test_predict_with_proba).numpy()\n",
    "\n",
    "def acc(forest, x_test, y_test):\n",
    "    y_test_predict = forest.predict(x_test)\n",
    "    return accuracy_score(y_test,y_test_predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accLossPercentage(forest, x, y, indices_of_test, accArr, lossArr, percentageArr=None, freqs=None,classToTest=None):\n",
    "    '''\n",
    "    This function fills the arrays of results accArr, lossArr and percentageArr for all the test items, or only the \n",
    "    negative items (classToTest=0) or only the positive items (classToTest=1). percentageArr is optional because it \n",
    "    is not required for the entire dataset, i.e., when we do not specify the class. \n",
    "    This works by using a dynamic programmation for the arrays. The return element is either the current accuracy in \n",
    "    case of classToTest=None, either the indices in case of classToTest!=None.\n",
    "    Params:\n",
    "    @forest: a trained model\n",
    "    @x_test: the dataset to test\n",
    "    @y_test: the target value to predict for the test dataset\n",
    "    @accArr: a list of the previous accurary, or an empty list\n",
    "    @lossArr: a list of the previous loss, or an empty list\n",
    "    @percentageArr:  a list of the previous percentage, or an empty list\n",
    "    @classToTest: 1 or 0 for positive and negative. This will find the indices of the items that belongs to the class\n",
    "    '''\n",
    "    if classToTest!=None:\n",
    "        indices = [i for i in indices_of_test if y[i][1]==classToTest]\n",
    "        if len(indices)>0:\n",
    "            accArr.append(acc(forest, x[indices], y[indices]))\n",
    "            lossArr.append(loss(forest, x[indices], y[indices]))\n",
    "        #percentageArr.append(freqs[indices].sum()/freqs[indices_of_test].sum())\n",
    "        percentageArr.append(len(indices)/len(indices_of_test))\n",
    "        return indices\n",
    "    else :\n",
    "        accuracy = acc(forest, x[indices_of_test], y[indices_of_test])\n",
    "        accArr.append(accuracy)\n",
    "        lossArr.append(loss(forest, x[indices_of_test], y[indices_of_test]))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runKFoldForRF(numberOfFold,x,y,packageForFitness,minLengthRun, m_AC):\n",
    "    accAll, lossAll = [], []\n",
    "    accNeg, lossNeg, percentageNeg = [], [], []\n",
    "    accPos, lossPos, percentagePos = [], [], []\n",
    "    \n",
    "    realFitness, realLBFitness, predictedLBFitness = [], [], []\n",
    "    packageForFitness.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    runtime = []\n",
    "\n",
    "    # use a K-fold Cross-Validation and show average Loss and average Accuracy\n",
    "    kfold = KFold(numberOfFold,)\n",
    "    bestmodel, bestAccuracy = None, 0\n",
    "    \n",
    "    for indices_of_train, indices_of_test in kfold.split(x):\n",
    "        forest = RandomForestClassifier(n_estimators = 100) \n",
    "        forest.fit(x[indices_of_train], y[indices_of_train])\n",
    "        \n",
    "        # compute loss and accuracy on the test items\n",
    "        current_accuracy = accLossPercentage(forest, x, y, indices_of_test, accAll, lossAll, )\n",
    "        \n",
    "        # compute loss and accuracy on the test items that are negatives\n",
    "        accLossPercentage(forest, x, y, indices_of_test, accNeg, lossNeg, percentageNeg,packageForFitness.freqs, 0)\n",
    "\n",
    "        # compute loss and accuracy on the test items that are positives\n",
    "        indices_of_pos = accLossPercentage(forest, x, y, indices_of_test, accPos, lossPos, percentagePos,packageForFitness.freqs, 1) \n",
    "\n",
    "        # compute fitness and lower-bound\n",
    "        realFitness.append(fitness(packageForFitness.iloc[indices_of_test], minLengthRun))\n",
    "        \n",
    "        # compute real LB, which depends on number of pos items\n",
    "        if len(indices_of_pos)>0:\n",
    "            realLBFitness.append(LB_fitness(packageForFitness.iloc[indices_of_test], minLengthRun, m_AC, indices_of_pos))\n",
    "        \n",
    "        # compute predicted LB fitness and runtime\n",
    "        start = time.time()\n",
    "        predictions = forest.predict(x[indices_of_test])\n",
    "        runtime.append((time.time()-start)/len(indices_of_test))\n",
    "        \n",
    "        indices_of_predicted_as_positives = [indices_of_test[i] for i in range(0,len(predictions)) if predictions[i][1]==1]\n",
    "        if len(indices_of_predicted_as_positives)>0:\n",
    "            predictedLBFitness.append(LB_fitness(packageForFitness.iloc[indices_of_test], minLengthRun, m_AC, indices_of_predicted_as_positives))\n",
    "            \n",
    "        if bestAccuracy < current_accuracy:\n",
    "            bestmodel = forest\n",
    "            bestAccuracy = current_accuracy\n",
    "            \n",
    "    print(\"[CROSS-VALIDATION]\\n[ALL] Loss:\", \"{:.3f}\".format(mean(lossAll)), \"\\t Acc:\", \"{:.3f}\".format(mean(accAll)))\n",
    "    print(\"[POSITIVE ({:.2f}%)] Loss:\".format(mean(percentagePos)), \"{:.3f}\".format(mean(lossPos)), \"\\t Acc:\", \"{:.3f}\".format(mean(accPos)))\n",
    "    print(\"[NEGATIVE ({:.2f}%)] Loss:\".format(mean(percentageNeg)), \"{:.3f}\".format(mean(lossNeg)),\"\\t Acc:\", \"{:.3f}\".format(mean(accNeg)))\n",
    "    print(\"Fitness {:.3f}\".format(mean(realFitness)), \"\\t LB Fitness:\", \"{:.3f}\\n\".format(mean(realLBFitness)),\"\\t LB Fitness:\", \"{:.3f}\\n\".format(mean(predictedLBFitness)))\n",
    "    print(\"Runtime (prediction per trace):{:.10f}\".format(mean(runtime)))\n",
    "    return bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.011 \t Acc: 0.997\n",
      "[POSITIVE (1.00%)] Loss: 0.003 \t Acc: 1.000\n",
      "[NEGATIVE (0.00%)] Loss: 2.006 \t Acc: 0.246\n",
      "Fitness 0.950 \t LB Fitness: 0.896\n",
      " \t LB Fitness: 0.898\n",
      "\n",
      "Runtime (prediction per trace):0.0000212629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of use\n",
    "runKFoldForRF(3,x,y,packageForFitness, minLengthRun, m_AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Test\n",
    "\n",
    "In this section, we launch the cross-validation on a training set and test with a different set, thanks to `train_test_split`. \n",
    "\n",
    "The loop gives the results for <b>different mAC values</b> from 1 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " m_AC= 2\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.011 \t Acc: 0.996\n",
      "[POSITIVE (0.08%)] Loss: 0.056 \t Acc: 0.982\n",
      "[NEGATIVE (0.92%)] Loss: 0.007 \t Acc: 0.997\n",
      "Fitness 0.836 \t LB Fitness: 0.074\n",
      " \t LB Fitness: 0.075\n",
      "\n",
      "Runtime (prediction per trace):0.0000602695\n",
      "(1441,)\n",
      "[TEST]\n",
      "[ALL] Loss: 0.008 \t Acc: 0.998\n",
      "[POSITIVE (0.08%)] Loss: 0.044 \t Acc: 1.000\n",
      "[NEGATIVE (0.92%)] Loss: 0.005 \t Acc: 0.998\n",
      "Fitness 0.837 \t LB Fitness: 0.071\n",
      " \t Predicted LB Fitness: 0.073\n",
      "\n",
      "[MOCK] Loss: 0.206 \t Acc: 0.912\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 4\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.020 \t Acc: 0.996\n",
      "[POSITIVE (0.20%)] Loss: 0.030 \t Acc: 0.994\n",
      "[NEGATIVE (0.80%)] Loss: 0.017 \t Acc: 0.997\n",
      "Fitness 0.836 \t LB Fitness: 0.174\n",
      " \t LB Fitness: 0.175\n",
      "\n",
      "Runtime (prediction per trace):0.0000651242\n",
      "(1441,)\n",
      "[TEST]\n",
      "[ALL] Loss: 0.023 \t Acc: 0.999\n",
      "[POSITIVE (0.20%)] Loss: 0.018 \t Acc: 1.000\n",
      "[NEGATIVE (0.80%)] Loss: 0.025 \t Acc: 0.998\n",
      "Fitness 0.837 \t LB Fitness: 0.169\n",
      " \t Predicted LB Fitness: 0.170\n",
      "\n",
      "[MOCK] Loss: 0.391 \t Acc: 0.873\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 6\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.157 \t Acc: 0.970\n",
      "[POSITIVE (0.56%)] Loss: 0.057 \t Acc: 0.988\n",
      "[NEGATIVE (0.44%)] Loss: 0.283 \t Acc: 0.948\n",
      "Fitness 0.836 \t LB Fitness: 0.462\n",
      " \t LB Fitness: 0.475\n",
      "\n",
      "Runtime (prediction per trace):0.0000610566\n",
      "(1441,)\n",
      "[TEST]\n",
      "[ALL] Loss: 0.126 \t Acc: 0.972\n",
      "[POSITIVE (0.58%)] Loss: 0.066 \t Acc: 0.978\n",
      "[NEGATIVE (0.42%)] Loss: 0.208 \t Acc: 0.962\n",
      "Fitness 0.837 \t LB Fitness: 0.476\n",
      " \t Predicted LB Fitness: 0.479\n",
      "\n",
      "[MOCK] Loss: 0.603 \t Acc: 0.843\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 8\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.055 \t Acc: 0.983\n",
      "[POSITIVE (0.64%)] Loss: 0.041 \t Acc: 0.985\n",
      "[NEGATIVE (0.36%)] Loss: 0.079 \t Acc: 0.978\n",
      "Fitness 0.836 \t LB Fitness: 0.490\n",
      " \t LB Fitness: 0.489\n",
      "\n",
      "Runtime (prediction per trace):0.0000655470\n",
      "(1441,)\n",
      "[TEST]\n",
      "[ALL] Loss: 0.085 \t Acc: 0.983\n",
      "[POSITIVE (0.65%)] Loss: 0.032 \t Acc: 0.989\n",
      "[NEGATIVE (0.35%)] Loss: 0.184 \t Acc: 0.970\n",
      "Fitness 0.837 \t LB Fitness: 0.500\n",
      " \t Predicted LB Fitness: 0.501\n",
      "\n",
      "[MOCK] Loss: 1.031 \t Acc: 0.777\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 10\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.077 \t Acc: 0.974\n",
      "[POSITIVE (0.71%)] Loss: 0.052 \t Acc: 0.980\n",
      "[NEGATIVE (0.29%)] Loss: 0.137 \t Acc: 0.960\n",
      "Fitness 0.836 \t LB Fitness: 0.513\n",
      " \t LB Fitness: 0.510\n",
      "\n",
      "Runtime (prediction per trace):0.0000565289\n",
      "(1441,)\n",
      "[TEST]\n",
      "[ALL] Loss: 0.102 \t Acc: 0.972\n",
      "[POSITIVE (0.73%)] Loss: 0.052 \t Acc: 0.980\n",
      "[NEGATIVE (0.27%)] Loss: 0.232 \t Acc: 0.949\n",
      "Fitness 0.837 \t LB Fitness: 0.524\n",
      " \t Predicted LB Fitness: 0.523\n",
      "\n",
      "[MOCK] Loss: 1.243 \t Acc: 0.650\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filename = \"A_2012_shm.csv\"\n",
    "\n",
    "for i in [2,4,6,8,10]:\n",
    "    print(\"\\n\",\"m_AC=\",i)\n",
    "    \n",
    "    # read the datafile, packageForFitness, minLengthRun, m_AC are needed to compute fitness and LBfitness\n",
    "    x, y, packageForFitness, minLengthRun, m_AC  = cleanDataForRF(\"alignments/\"+filename,i)\n",
    "    \n",
    "    # split the dataset in TRAIN and TEST sets\n",
    "    X_train, X_test, y_train, y_test, packageForFitness_train, packageForFitness_test = train_test_split(x, y, packageForFitness, test_size=0.33, random_state=42)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    #                     TRAIN \n",
    "    # ----------------------------------------------\n",
    "    # run the cross-validation\n",
    "    bestmodel = runKFoldForRF(10, X_train, y_train, packageForFitness_train, minLengthRun, m_AC)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    #                     TEST \n",
    "    # ----------------------------------------------\n",
    "    # use the same function as in the cross-validation but for the test set. \n",
    "    accAll, lossAll = [], []\n",
    "    accNeg, lossNeg, percentageNeg   = [], [], []\n",
    "    accPos, lossPos, percentagePos = [], [], []\n",
    "    \n",
    "    packageForFitness_test.reset_index(drop=True,inplace=True)\n",
    "    print(packageForFitness_test.freqs.shape)\n",
    "    #compute loss and accuracy on the test items \n",
    "    accLossPercentage(bestmodel, X_test, y_test, list(range(0,len(y_test))), accAll, lossAll)\n",
    "        \n",
    "    # compute loss and accuracy on the test items that are negatives\n",
    "    accLossPercentage(bestmodel, X_test, y_test, list(range(0,len(y_test))), accNeg, lossNeg, percentageNeg,packageForFitness_test.freqs, 0)\n",
    "\n",
    "    # compute loss and accuracy on the test items that are positives\n",
    "    indices_of_pos = accLossPercentage(bestmodel, X_test, y_test, list(range(0,len(y_test))), accPos, lossPos, percentagePos,packageForFitness_test.freqs, 1) \n",
    "\n",
    "    # compute fitness and lower-bound\n",
    "    realFitness = fitness(packageForFitness_test, minLengthRun)\n",
    "    if len(indices_of_pos)>0:\n",
    "        realLBFitness = LB_fitness(packageForFitness_test, minLengthRun, m_AC, indices_of_pos)\n",
    "        \n",
    "    # compute predicted LB fitness\n",
    "    predictions = bestmodel.predict(X_test)\n",
    "    indices_of_predicted_as_positives = [i for i in range(0, len(X_test)) if predictions[i][1]==1]\n",
    "    if len(indices_of_predicted_as_positives)>0:\n",
    "        predictedLBFitness = LB_fitness(packageForFitness_test, minLengthRun, m_AC, indices_of_predicted_as_positives)\n",
    "        \n",
    "    print(\"[TEST]\\n[ALL] Loss:\", \"{:.3f}\".format(mean(lossAll)), \"\\t Acc:\", \"{:.3f}\".format(mean(accAll)))\n",
    "    print(\"[POSITIVE ({:.2f}%)] Loss:\".format(mean(percentagePos)), \"{:.3f}\".format(mean(lossPos)), \"\\t Acc:\", \"{:.3f}\".format(mean(accPos)))\n",
    "    print(\"[NEGATIVE ({:.2f}%)] Loss:\".format(mean(percentageNeg)), \"{:.3f}\".format(mean(lossNeg)),\"\\t Acc:\", \"{:.3f}\".format(mean(accNeg)))\n",
    "    print(\"Fitness {:.3f}\".format((realFitness)), \"\\t LB Fitness:\", \"{:.3f}\\n\".format((realLBFitness)),\"\\t Predicted LB Fitness:\", \"{:.3f}\\n\".format((predictedLBFitness)))\n",
    "    \n",
    "    fake_x, fake_y, fake_packageForFitness, fake_minLengthRun, fake_m_AC  = cleanDataForRF(\"alignments/mock/\"+filename,i,vectorizer_already_trained=True)\n",
    "\n",
    "    accFake, lossFake = [], []\n",
    "    accLossPercentage(bestmodel, fake_x, fake_y, range(0,len(fake_y)), accFake, lossFake)\n",
    "    print(\"[MOCK] Loss:\", \"{:.3f}\".format(mean(lossFake)), \"\\t Acc:\", \"{:.3f}\".format(mean(accFake)))\n",
    "    \n",
    "    print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traces: 4366 \n",
      "Max len of traces: 176\n"
     ]
    }
   ],
   "source": [
    "# ---- (3) prepare the sequences of activities \n",
    "data = pd.read_csv(\"alignments/\"+filename,sep = \";\", \n",
    "                   names = [\"traces\",\"tracesWithMoves\",\"runs\",\"runsWithMoves\",\"costs\",\"frequencies\"])\n",
    "\n",
    "traces_to_matrix = data.traces.str.split(\":::\",expand=True,)\n",
    "number_of_traces, max_len = traces_to_matrix.shape\n",
    "print(\"Number of traces:\", number_of_traces, \"\\nMax len of traces:\", max_len) \n",
    "\n",
    "# transform the matrix to a serie\n",
    "traces_to_serie = pd.concat([traces_to_matrix[i] for i in range(0,max_len)], axis=0, \n",
    "                                      ignore_index=True, sort=False)\n",
    "# from the serie, it's easy to get unique words\n",
    "index_to_word = list(filter(None,(traces_to_serie).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A_SUBMITTED', 0.0)\n",
      "('A_PARTLYSUBMITTED', 0.0)\n",
      "('A_DECLINED', 0.00010898079012263087)\n",
      "('W_Afhandelen leads', 0.00017831727950181467)\n",
      "('A_PREACCEPTED', 0.00019110727475472446)\n",
      "('W_Beoordelen fraude', 0.0009586118224845017)\n",
      "('W_Completeren aanvraag', 0.0031592086765537524)\n",
      "('A_ACCEPTED', 0.0034073114942701956)\n",
      "('A_CANCELLED', 0.006056163205674222)\n",
      "('O_SELECTED', 0.007134492506164547)\n",
      "('A_FINALIZED', 0.007594912974757332)\n",
      "('O_CREATED', 0.009294955624499479)\n",
      "('O_SENT', 0.009479609926060423)\n",
      "('W_Nabellen offertes', 0.009988496815615235)\n",
      "('O_SENT_BACK', 0.014086831185175048)\n",
      "('O_CANCELLED', 0.01463934913061405)\n",
      "('O_DECLINED', 0.019868295099805436)\n",
      "('W_Valideren aanvraag', 0.02332422625368348)\n",
      "('O_ACCEPTED', 0.025264931068749087)\n",
      "('W_Nabellen incomplete dossiers', 0.027799145415412074)\n",
      "('A_APPROVED', 0.03182374264532737)\n",
      "('A_REGISTERED', 0.04446629556848834)\n",
      "('A_ACTIVATED', 0.25747534402240163)\n",
      "('W_Wijzigen contractgegevens', 0.4836996712198847)\n"
     ]
    }
   ],
   "source": [
    "for feature in zip(index_to_word, np.sort(bestmodel.feature_importances_)):\n",
    "    print(feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
