{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for An Alignment Cost-Based Classification of Log Traces\n",
    "\n",
    "> Paper : <u>An Alignment Cost-Based Classification of Log Traces </u><br>\n",
    "> Date : June 2020 <br>\n",
    "> Authors : <i>Mathilde Boltenhagen, Benjamin Chetioui, and Laurine Huber  </i> <br>\n",
    "\n",
    "This notebook is organized as follow : <br> <br>\n",
    "<b>0. Fitness function </b> \n",
    "- The lower bound fitness is a good contribution of the paper, please see the paper for more details. <br>\n",
    "\n",
    "<b>1. Preprocessing the data:</b>\n",
    " - A function ```cleanDataForRNN``` contains all the preprocessing steps. It reads the file, create the sequences and the targets. \n",
    " \n",
    "<b>2. Model: </b> \n",
    "   - The model is a bi-RNN with different layers. \n",
    "   \n",
    "<b>3. Cross-Validation of the method : </b>\n",
    " - A function ```runKFoldForRNN``` runs a Kfold method to fit and test the model on the sequences\n",
    " \n",
    "<b> 4. Train and Test :</b>\n",
    "- Train and test for many  m_AC values\n",
    "\n",
    "## 0. Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import time\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commented lines were used when the frequency of the variants was incorporated in the computation of fitness.\n",
    "In fact, we decided to compute the fitness and lower bound for fitness on the variants only due to the understanding of the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(packageForFitness, minRunLength):\n",
    "    '''\n",
    "    This function computes the fitness for all the sequences. \n",
    "    @sequences: the sequences of words\n",
    "    @costs: the real alignment cost\n",
    "    @minRunLength: the minimal run in the alignment dataset\n",
    "    '''\n",
    "    sumTraceFitness = 0\n",
    "    totTraces = 0 \n",
    "    for i in packageForFitness.index:\n",
    "        sumTraceFitness += (1 - (packageForFitness.realCosts[i] / ( packageForFitness.lengths[i]  + minRunLength )))#*packageForFitness.freqs[i]\n",
    "        #totTraces += packageForFitness.freqs[i]\n",
    "        totTraces += 1\n",
    "    return sumTraceFitness / totTraces\n",
    "\n",
    "def LB_fitness(packageForFitness, minRunLength, m_AC, indices):\n",
    "    '''\n",
    "    This function computes lower bound of the fitness given in the paper. \n",
    "    @sequences: the sequences of words\n",
    "    @minRunLength: the minimal run in the alignment dataset\n",
    "    @m_AC: needed for the lowerbound formula\n",
    "    @indices: if we compute the lower bound, then we don't iterate on all the traces but only the positives. \n",
    "    '''\n",
    "    sumTraceFitness = 0\n",
    "    totTraces = 0 \n",
    "    for i in indices:\n",
    "        sumTraceFitness += (1 - ((m_AC) / ( packageForFitness.lengths[i] + minRunLength )))#*packageForFitness.freqs[i]\n",
    "    for i in packageForFitness.index:\n",
    "        #totTraces += packageForFitness.freqs[i]\n",
    "        totTraces += 1\n",
    "    return sumTraceFitness / totTraces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the data\n",
    "\n",
    "This function takes as input an alignment dataset and its Maximal Alignment Cost and clean the data in order to get the sequences and the target classes. Please, see the definition of Maximal Alignment Cost Classification for more details on the target classes. The sequences are the sequences of indices from a dictionary that store an index per word (index_to_word is the inverse mapping). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be a global variable for fake data\n",
    "word_to_index = {}\n",
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataForRNN(dataFile,m_AC, word_to_index_already_exists=None):\n",
    "    '''\n",
    "    Reads the file (1), specifies the target classes (2) and prepare the sequences of activities (3). \n",
    "    Finally, its prepare some variables `sequences`, `costs` and `minRunLenght` for computing fitness. \n",
    "    @dataFile: (String) filename of the alignment dataset\n",
    "    @m_AC: (int) maximal alignment cost classifier\n",
    "    '''\n",
    "    # ---- (1) read the file \n",
    "    data = pd.read_csv(dataFile,sep = \";\", \n",
    "                   names = [\"traces\",\"tracesWithMoves\",\"runs\",\"runsWithMoves\",\"costs\",\"frequencies\"])\n",
    "    \n",
    "    # ---- (2) create the positive and negative target depending on the m_AC parameter\n",
    "    # alignment cost which interests us is greater than 10000 (other costs are just silent moves)\n",
    "    # set the fitting to tmp_pos to set them latter to 1\n",
    "    y = (((data[\"costs\"] / 10000).astype(int)) / (m_AC+1)).astype(int)\n",
    "    max_y = y.max()\n",
    "    y = y.replace(0,\"tmp_pos\")\n",
    "    y = y.replace(range(1,max_y + 1), 0)\n",
    "    y = y.replace(\"tmp_pos\",1)\n",
    "\n",
    "    # two columns are required for a binary classification in a RNN\n",
    "    y = np.eye(2)[y.to_numpy().reshape(-1)]\n",
    "    \n",
    "    # ---- (3) prepare the sequences of activities \n",
    "    traces_to_matrix = data.traces.str.split(\":::\",expand=True,)\n",
    "    if word_to_index_already_exists== None:\n",
    "        global max_len\n",
    "        number_of_traces, max_len = traces_to_matrix.shape\n",
    "    \n",
    "    # transform the matrix to a serie\n",
    "    traces_to_serie = pd.concat([traces_to_matrix[i] for i in range(0,traces_to_matrix.shape[1])], axis=0, \n",
    "                                          ignore_index=True, sort=False)\n",
    "    # from the serie, it's easy to get unique words\n",
    "    index_to_word = list(filter(None,(traces_to_serie).unique()))\n",
    "    number_of_activities = len(index_to_word)\n",
    "\n",
    "    # a simple dictionary \n",
    "    if word_to_index_already_exists==None:\n",
    "        global word_to_index \n",
    "        word_to_index = { index_to_word[i]:i+1 for i in range(0, len(index_to_word) ) }\n",
    "    \n",
    "    # loop over traces \n",
    "    x = np.zeros((data.traces.shape[0],max_len))   \n",
    "    for i in range(data.traces.shape[0]):\n",
    "        # Convert the ith training trace and split is into activities. \n",
    "        trace = data.traces[i].split(\":::\")[:-1]\n",
    "        # for every activity name in ith trace, jth is its index /!\\ \n",
    "        j = 0\n",
    "        # Loop over the activities \n",
    "        for w in trace:\n",
    "            # Set the (i,j)th entry of X_trains to the index of the correct word.\n",
    "            x[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j += 1   \n",
    "    \n",
    "    # for fitness computation\n",
    "    minLengthRun= len(data.runs.str.split(\":::\").min())\n",
    "    lengths = data.traces.str.split(\":::\",expand=False,).str.len()-1\n",
    "    realCosts = (data[\"costs\"] / 10000).astype(int)\n",
    "    packageForFitness = pd.DataFrame({\"lengths\":lengths, \"realCosts\": realCosts, \"freqs\": data.frequencies})         \n",
    "    \n",
    "    return x, y, number_of_activities, max_len, index_to_word, packageForFitness, minLengthRun, m_AC   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2., 3., ..., 0., 0., 0.],\n",
       "        [1., 2., 4., ..., 0., 0., 0.],\n",
       "        [1., 2., 4., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 2., 5., ..., 0., 0., 0.],\n",
       "        [1., 2., 5., ..., 0., 0., 0.],\n",
       "        [1., 2., 4., ..., 0., 0., 0.]]), array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of use\n",
    "x, y, number_of_activities, max_len, index_to_word, packageForFitness, minLengthRun, m_AC  = cleanDataForRNN(\"alignments/A_2012_im.csv\",5)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "Find below a function that creates the model instance depending on the size of the dictionary and the max length of a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myModel(max_len, number_of_activities, verbose=True):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    size_of_voc -- size of the embedding output\n",
    "    \n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    # Input of the model, type is dtype 'int32' (as it contains indices of activities, which are integers).\n",
    "    indices = Input(shape= (max_len,), dtype='int32')\n",
    "    # Create the embedding layer to reduce to voc size\n",
    "    embeddings = Embedding(input_dim= number_of_activities, output_dim= 15, input_length=max_len)(indices)\n",
    "    # Propagate the embeddings through an Bi-LSTM layer with 50-dimensional hidden state \n",
    "    # Return the batch of sequences (to replay)\n",
    "    X = Bidirectional(LSTM(units = 50, return_sequences=True, go_backwards=True),merge_mode='concat')(embeddings)\n",
    "    # Add dropout with a probability of 0.5 to help with overfitting\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 50-dimensional hidden state\n",
    "    X = LSTM(units = 50, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer of the number of predicting classes\n",
    "    X = Dense(units=2)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts indices into X.\n",
    "    model = Model(inputs=indices, outputs=X)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 176, 15)           360       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 176, 100)          26400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 176, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 57,062\n",
      "Trainable params: 57,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# example of use\n",
    "model = myModel(max_len, number_of_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Cross-Validation of the method \n",
    "\n",
    "From ```KFold``` of sklearn, we do a cross-validation. The outputs are average of the accuracy and the loss (binary-cross entropy). We use the ```binary_crossentropy``` function of <b> Tensorflow </b>. \n",
    "\n",
    "- Notice that the RNN is also doing a validation slipt of 33% of the training set. \n",
    "\n",
    "- In order to see if positives or negatives items have better results, we give the results for `all` the test items, only the `positive` items and only the `negative` items. \n",
    "\n",
    "verbose=0 for silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accLossPercentageForRNN(model, x, y, indices_of_test, accArr, lossArr, percentageArr=None,  classToTest=None, verbose=2,freqs=None):\n",
    "    '''\n",
    "    This function fills the arrays of results accArr, lossArr and percentageArr for all the test items, or only the \n",
    "    negative items (classToTest=0) or only the positive items (classToTest=1). percentageArr is optional because it \n",
    "    is not required for the entire dataset, i.e., when we do not specify the class. \n",
    "    This works by using a dynamic programmation for the arrays. The return element is either the current accuracy in \n",
    "    case of classToTest=None, either the indices in case of classToTest!=None.\n",
    "    Params:\n",
    "    @forest: a trained model\n",
    "    @x_test: the dataset to test\n",
    "    @y_test: the target value to predict for the test dataset\n",
    "    @accArr: a list of the previous accurary, or an empty list\n",
    "    @lossArr: a list of the previous loss, or an empty list\n",
    "    @percentageArr:  a list of the previous percentage, or an empty list\n",
    "    @classToTest: 1 or 0 for positive and negative. This will find the indices of the items that belongs to the class\n",
    "    '''\n",
    "    if classToTest!=None:\n",
    "        indices = [i for i in indices_of_test if y[i][1]==classToTest]\n",
    "        if len(indices)>0:\n",
    "            loss, acc = model.evaluate(x[indices], y[indices],verbose=verbose)\n",
    "            accArr.append(acc)\n",
    "            lossArr.append(loss)\n",
    "        #percentageArr.append(freqs[indices].sum()/freqs[indices_of_test].sum())\n",
    "        percentageArr.append(len(indices)/len(indices_of_test))\n",
    "        return indices\n",
    "    else :\n",
    "        loss, acc = model.evaluate(x[indices_of_test], y[indices_of_test],verbose=verbose)\n",
    "        accArr.append(acc)\n",
    "        lossArr.append(loss)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runKFoldForRNN(numberOfFold, x, y, packageForFitness,minLengthRun, m_AC,\n",
    "                   max_len, number_of_activities, verbose=2,epochs = 10, batch_size = 50):\n",
    "    '''\n",
    "    This function runs a RNN and prints some metrics. \n",
    "    \n",
    "    @model: the created RNN\n",
    "    @numberOfFold: number of fold for the cross validation\n",
    "    @x: sequences\n",
    "    @y: target\n",
    "    '''    \n",
    "    startModel = time.time()\n",
    "    accAll, lossAll = [], []\n",
    "    accNeg, lossNeg, percentageNeg = [], [], []\n",
    "    accPos, lossPos, percentagePos = [], [], []\n",
    "    \n",
    "    realFitness, realLBFitness, predictedLBFitness = [], [], []\n",
    "    packageForFitness.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    runtime = []\n",
    "\n",
    "    # use a K-fold Cross-Validation and show average Loss and average Accuracy\n",
    "    kfold = KFold(numberOfFold)\n",
    "    bestmodel, bestAccuracy = None, 0\n",
    "    \n",
    "    for indices_of_train, indices_of_test in kfold.split(x):\n",
    "        \n",
    "        # train the model\n",
    "        model = myModel(max_len, number_of_activities+1,verbose)\n",
    "        model.fit(x[indices_of_train], y[indices_of_train], verbose=verbose, epochs = 10, batch_size = 50)\n",
    "        \n",
    "        # compute loss and accuracy on the test items\n",
    "        current_accuracy = accLossPercentageForRNN(model, x, y, indices_of_test, accAll, lossAll, verbose=verbose)\n",
    "        \n",
    "        # compute loss and accuracy on the test items that are negatives\n",
    "        accLossPercentageForRNN(model, x, y, indices_of_test, accNeg, lossNeg, percentageNeg, 0, verbose)\n",
    "\n",
    "        # compute loss and accuracy on the test items that are positives\n",
    "        indices_of_pos = accLossPercentageForRNN(model, x, y, indices_of_test, accPos, lossPos, percentagePos, 1, verbose) \n",
    "\n",
    "        # compute fitness and lower-bound\n",
    "        realFitness.append(fitness(packageForFitness.iloc[indices_of_test], minLengthRun))\n",
    "        if len(indices_of_pos)>0:\n",
    "            realLBFitness.append(LB_fitness(packageForFitness.iloc[indices_of_test], minLengthRun, m_AC, indices_of_pos))\n",
    "        \n",
    "        # compute predicted LB fitness and runtime\n",
    "        start = time.time()\n",
    "        predictions = model.predict(x[indices_of_test])\n",
    "        runtime.append((time.time()-start)/len(indices_of_test))\n",
    "        indices_of_predicted_as_positives = [indices_of_test[i] for i in range(0,len(predictions)) if predictions[i][1]>=0.5]\n",
    "        if len(indices_of_predicted_as_positives)>0:\n",
    "            predictedLBFitness.append(LB_fitness(packageForFitness.iloc[indices_of_test], minLengthRun, m_AC, indices_of_predicted_as_positives))\n",
    "\n",
    "        if bestAccuracy < current_accuracy:\n",
    "            bestmodel = model\n",
    "            bestAccuracy = current_accuracy\n",
    "    print(\"TIME:\",time.time()-startModel)   \n",
    "    print(\"[CROSS-VALIDATION]\\n[ALL] Loss:\", \"{:.3f}\".format(mean(lossAll)), \"\\t Acc:\", \"{:.3f}\".format(mean(accAll)))\n",
    "    print(\"[POSITIVE ({:.2f}%)] Loss:\".format(mean(percentagePos)), \"{:.3f}\".format(mean(lossPos)), \"\\t Acc:\", \"{:.3f}\".format(mean(accPos)))\n",
    "    print(\"[NEGATIVE ({:.2f}%)] Loss:\".format(mean(percentageNeg)), \"{:.3f}\".format(mean(lossNeg)),\"\\t Acc:\", \"{:.3f}\".format(mean(accNeg)))\n",
    "    print(\"Fitness {:.3f}\".format(mean(realFitness)), \"\\t LB Fitness:\", \"{:.3f}\\n\".format(mean(realLBFitness)),\"\\t Predicted LB Fitness:\", \"{:.3f}\\n\".format(mean(predictedLBFitness)))\n",
    "    print(\"Runtime (prediction per trace):{:.10f}\".format(mean(runtime)))\n",
    "    return bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 176, 15)           375       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 176, 100)          26400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 176, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 57,077\n",
      "Trainable params: 57,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 26s - loss: 0.1331 - acc: 0.9900\n",
      "Epoch 2/10\n",
      " - 23s - loss: 0.0058 - acc: 0.9993\n",
      "Epoch 3/10\n",
      " - 22s - loss: 0.0068 - acc: 0.9993\n",
      "Epoch 4/10\n",
      " - 23s - loss: 0.0071 - acc: 0.9993\n",
      "Epoch 5/10\n",
      " - 23s - loss: 0.0074 - acc: 0.9993\n",
      "Epoch 6/10\n",
      " - 23s - loss: 0.0062 - acc: 0.9993\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.0064 - acc: 0.9993\n",
      "Epoch 8/10\n",
      " - 22s - loss: 0.0058 - acc: 0.9993\n",
      "Epoch 9/10\n",
      " - 23s - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 10/10\n",
      " - 23s - loss: 0.0071 - acc: 0.9993\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 176, 15)           375       \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 176, 100)          26400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 176, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 57,077\n",
      "Trainable params: 57,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 25s - loss: 0.1224 - acc: 0.9966\n",
      "Epoch 2/10\n",
      " - 23s - loss: 0.0081 - acc: 0.9993\n",
      "Epoch 3/10\n",
      " - 24s - loss: 0.0083 - acc: 0.9993\n",
      "Epoch 4/10\n",
      " - 25s - loss: 0.0070 - acc: 0.9993\n",
      "Epoch 5/10\n",
      " - 24s - loss: 0.0075 - acc: 0.9993\n",
      "Epoch 6/10\n",
      " - 22s - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.0062 - acc: 0.9993\n",
      "Epoch 8/10\n",
      " - 22s - loss: 0.0063 - acc: 0.9993\n",
      "Epoch 9/10\n",
      " - 22s - loss: 0.0062 - acc: 0.9993\n",
      "Epoch 10/10\n",
      " - 22s - loss: 0.0071 - acc: 0.9993\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 176, 15)           375       \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 176, 100)          26400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 176, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 102       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 57,077\n",
      "Trainable params: 57,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 25s - loss: 0.1298 - acc: 0.9935\n",
      "Epoch 2/10\n",
      " - 25s - loss: 0.0076 - acc: 0.9993\n",
      "Epoch 3/10\n",
      " - 24s - loss: 0.0074 - acc: 0.9993\n",
      "Epoch 4/10\n",
      " - 24s - loss: 0.0069 - acc: 0.9993\n",
      "Epoch 5/10\n",
      " - 25s - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 6/10\n",
      " - 26s - loss: 0.0073 - acc: 0.9993\n",
      "Epoch 7/10\n",
      " - 25s - loss: 0.0052 - acc: 0.9993\n",
      "Epoch 8/10\n",
      " - 23s - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 9/10\n",
      " - 28s - loss: 0.0063 - acc: 0.9993\n",
      "Epoch 10/10\n",
      " - 27s - loss: 0.0055 - acc: 0.9993\n",
      "TIME: 749.6055788993835\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.006 \t Acc: 0.999\n",
      "[POSITIVE (1.00%)] Loss: 0.000 \t Acc: 1.000\n",
      "[NEGATIVE (0.00%)] Loss: 8.421 \t Acc: 0.000\n",
      "Fitness 0.950 \t LB Fitness: 0.874\n",
      " \t Predicted LB Fitness: 0.875\n",
      "\n",
      "Runtime (prediction per trace):0.0021612150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x12db95dd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of use\n",
    "runKFoldForRNN(3,x,y, packageForFitness,minLengthRun, m_AC,max_len, number_of_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Test\n",
    "\n",
    "In this section, we launch the cross-validation on a training set and test with a different set, thanks to `train_test_split`. \n",
    "\n",
    "The loop gives the results for <b>different mAC values</b> from 1 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " m_AC= 2\n",
      "TIME: 2458.9817349910736\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.027 \t Acc: 0.989\n",
      "[POSITIVE (0.08%)] Loss: 0.217 \t Acc: 0.929\n",
      "[NEGATIVE (0.92%)] Loss: 0.012 \t Acc: 0.994\n",
      "Fitness 0.836 \t LB Fitness: 0.074\n",
      " \t Predicted LB Fitness: 0.073\n",
      "\n",
      "Runtime (prediction per trace):0.0076604263\n",
      "[TEST]\n",
      "[ALL] Loss: 0.023 \t Acc: 0.997\n",
      "[POSITIVE (0.08%)] Loss: 0.185 \t Acc: 0.982\n",
      "[NEGATIVE (0.92%)] Loss: 0.009 \t Acc: 0.998\n",
      "Fitness 0.837 \t LB Fitness: 0.071\n",
      " \t Predicted LB Fitness: 0.072\n",
      "\n",
      "[FAKE] Loss: 0.141 \t Acc: 0.959\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 4\n",
      "TIME: 2538.729465007782\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.064 \t Acc: 0.976\n",
      "[POSITIVE (0.20%)] Loss: 0.218 \t Acc: 0.917\n",
      "[NEGATIVE (0.80%)] Loss: 0.025 \t Acc: 0.991\n",
      "Fitness 0.836 \t LB Fitness: 0.174\n",
      " \t Predicted LB Fitness: 0.165\n",
      "\n",
      "Runtime (prediction per trace):0.0130540386\n",
      "[TEST]\n",
      "[ALL] Loss: 0.047 \t Acc: 0.990\n",
      "[POSITIVE (0.20%)] Loss: 0.111 \t Acc: 0.968\n",
      "[NEGATIVE (0.80%)] Loss: 0.031 \t Acc: 0.996\n",
      "Fitness 0.837 \t LB Fitness: 0.169\n",
      " \t Predicted LB Fitness: 0.166\n",
      "\n",
      "[FAKE] Loss: 0.339 \t Acc: 0.934\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 6\n",
      "TIME: 2546.310353040695\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.120 \t Acc: 0.962\n",
      "[POSITIVE (0.56%)] Loss: 0.067 \t Acc: 0.990\n",
      "[NEGATIVE (0.44%)] Loss: 0.188 \t Acc: 0.926\n",
      "Fitness 0.836 \t LB Fitness: 0.462\n",
      " \t Predicted LB Fitness: 0.484\n",
      "\n",
      "Runtime (prediction per trace):0.0210512503\n",
      "[TEST]\n",
      "[ALL] Loss: 0.114 \t Acc: 0.971\n",
      "[POSITIVE (0.58%)] Loss: 0.100 \t Acc: 0.992\n",
      "[NEGATIVE (0.42%)] Loss: 0.133 \t Acc: 0.943\n",
      "Fitness 0.837 \t LB Fitness: 0.476\n",
      " \t Predicted LB Fitness: 0.492\n",
      "\n",
      "[FAKE] Loss: 0.446 \t Acc: 0.879\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 8\n",
      "TIME: 2428.300120830536\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.075 \t Acc: 0.977\n",
      "[POSITIVE (0.64%)] Loss: 0.080 \t Acc: 0.976\n",
      "[NEGATIVE (0.36%)] Loss: 0.066 \t Acc: 0.980\n",
      "Fitness 0.836 \t LB Fitness: 0.490\n",
      " \t Predicted LB Fitness: 0.483\n",
      "\n",
      "Runtime (prediction per trace):0.0224502227\n",
      "[TEST]\n",
      "[ALL] Loss: 0.091 \t Acc: 0.975\n",
      "[POSITIVE (0.65%)] Loss: 0.089 \t Acc: 0.974\n",
      "[NEGATIVE (0.35%)] Loss: 0.095 \t Acc: 0.976\n",
      "Fitness 0.837 \t LB Fitness: 0.500\n",
      " \t Predicted LB Fitness: 0.493\n",
      "\n",
      "[FAKE] Loss: 1.064 \t Acc: 0.829\n",
      "-------------------------------------------\n",
      "\n",
      " m_AC= 10\n",
      "TIME: 2785.54851603508\n",
      "[CROSS-VALIDATION]\n",
      "[ALL] Loss: 0.153 \t Acc: 0.945\n",
      "[POSITIVE (0.71%)] Loss: 0.128 \t Acc: 0.937\n",
      "[NEGATIVE (0.29%)] Loss: 0.209 \t Acc: 0.966\n",
      "Fitness 0.836 \t LB Fitness: 0.513\n",
      " \t Predicted LB Fitness: 0.485\n",
      "\n",
      "Runtime (prediction per trace):0.0335029143\n",
      "[TEST]\n",
      "[ALL] Loss: 0.158 \t Acc: 0.943\n",
      "[POSITIVE (0.73%)] Loss: 0.104 \t Acc: 0.947\n",
      "[NEGATIVE (0.27%)] Loss: 0.302 \t Acc: 0.931\n",
      "Fitness 0.837 \t LB Fitness: 0.524\n",
      " \t Predicted LB Fitness: 0.508\n",
      "\n",
      "[FAKE] Loss: 2.996 \t Acc: 0.556\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filename = \"A_2012_shm.csv\"\n",
    "\n",
    "for i in [2,4,6,8,10]:\n",
    "    print(\"\\n\",\"m_AC=\",i)\n",
    "    \n",
    "    # read the datafile, packageForFitness, minLengthRun, m_AC are needed to compute fitness and LBfitness\n",
    "    x, y, number_of_activities, max_len, index_to_word, packageForFitness, minLengthRun, m_AC  = cleanDataForRNN(\"alignments/\"+filename,i)\n",
    "    \n",
    "    # split the dataset in TRAIN and TEST sets\n",
    "    X_train, X_test, y_train, y_test, packageForFitness_train, packageForFitness_test = train_test_split(x, y, packageForFitness, test_size=0.33, random_state=42)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    #                     TRAIN \n",
    "    # ----------------------------------------------\n",
    "    # run the cross-validation\n",
    "    bestmodel = runKFoldForRNN(10, X_train, y_train, packageForFitness_train, minLengthRun, m_AC, max_len, number_of_activities, verbose=0)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    #                     TEST \n",
    "    # ----------------------------------------------\n",
    "    # use the same function as in the cross-validation but for the test set. \n",
    "    accAll, lossAll = [], []\n",
    "    accNeg, lossNeg, percentageNeg = [], [], []\n",
    "    accPos, lossPos, percentagePos = [], [], []\n",
    "    \n",
    "    packageForFitness_test.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    #compute loss and accuracy on the test items \n",
    "    accLossPercentageForRNN(bestmodel, X_test, y_test, range(0,len(y_test)), accAll, lossAll, verbose=0)\n",
    "        \n",
    "    # compute loss and accuracy on the test items that are negatives\n",
    "    accLossPercentageForRNN(bestmodel, X_test, y_test, range(0,len(y_test)), accNeg, lossNeg, percentageNeg, 0, 0)\n",
    "\n",
    "    # compute loss and accuracy on the test items that are positives\n",
    "    indices_of_pos = accLossPercentageForRNN(bestmodel, X_test, y_test, range(0,len(y_test)), accPos, lossPos,percentagePos,  1, 0) \n",
    "\n",
    "    # compute fitness and lower-bound\n",
    "    realFitness = fitness(packageForFitness_test, minLengthRun)\n",
    "    if len(indices_of_pos)>0:\n",
    "        LBFitness = LB_fitness(packageForFitness_test, minLengthRun, m_AC, indices_of_pos)\n",
    "        \n",
    "    # compute predicted LB fitness\n",
    "    predictions = bestmodel.predict(X_test)\n",
    "    indices_of_predicted_as_positives = [i for i in range(0, len(X_test)) if predictions[i][1]>=0.5]\n",
    "    if len(indices_of_predicted_as_positives)>0:\n",
    "        predictedLBFitness = LB_fitness(packageForFitness_test, minLengthRun, m_AC, indices_of_predicted_as_positives)\n",
    "        \n",
    "    print(\"[TEST]\\n[ALL] Loss:\", \"{:.3f}\".format(mean(lossAll)), \"\\t Acc:\", \"{:.3f}\".format(mean(accAll)))\n",
    "    print(\"[POSITIVE ({:.2f}%)] Loss:\".format(mean(percentagePos)), \"{:.3f}\".format(mean(lossPos)), \"\\t Acc:\", \"{:.3f}\".format(mean(accPos)))\n",
    "    print(\"[NEGATIVE ({:.2f}%)] Loss:\".format(mean(percentageNeg)), \"{:.3f}\".format(mean(lossNeg)),\"\\t Acc:\", \"{:.3f}\".format(mean(accNeg)))\n",
    "    print(\"Fitness {:.3f}\".format((realFitness)), \"\\t LB Fitness:\", \"{:.3f}\\n\".format((LBFitness)),\"\\t Predicted LB Fitness:\", \"{:.3f}\\n\".format((predictedLBFitness)) )\n",
    "\n",
    "    fake_x, fake_y, fake_number_of_activities, fake_max_len, fake_index_to_word, fake_packageForFitness, fake_minLengthRun, fake_m_AC  = cleanDataForRNN(\"alignments/mock/\"+filename,2,word_to_index_already_exists=True)\n",
    "    accFake, lossFake = [], []\n",
    "    accLossPercentageForRNN(bestmodel, fake_x, fake_y, range(0,len(fake_y)), accFake, lossFake)\n",
    "    print(\"[MOCK] Loss:\", \"{:.3f}\".format(mean(lossFake)), \"\\t Acc:\", \"{:.3f}\".format(mean(accFake)))\n",
    "    print(\"-------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
